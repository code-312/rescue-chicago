{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from config import AGE_DICT, SIZE_DICT, TARGET_COLS, BINARY_COLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEROKU_URL = os.getenv('HEROKU_POSTGRESQL_AMBER_URL')\n",
    "\n",
    "uri = HEROKU_URL \n",
    "if uri.startswith(\"postgres://\"):\n",
    "    uri = uri.replace(\"postgres://\", \"postgresql://\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "def read_data():\n",
    "    df_raw = df_raw = pd.read_sql('petfinder_with_dates', uri)  \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "\n",
    "    # dropping irrelevant columns\n",
    "    df = df.drop(columns=[\"id\", \"name\", \"organization_id\", \"published_at\", \"status_changed_at\", \"attribute_declawed\", \"color_tertiary\", \"good_with_cats\", \"good_with_children\", \"good_with_dogs\", \"breed_secondary\", \"color_secondary\"])\n",
    "\n",
    "    # transform \"age\" column \n",
    "    df['age'] = df['age'].map(AGE_DICT).astype(str).astype(int)\n",
    "\n",
    "    # transform \"size\" column\n",
    "    df['size'] = df['size'].map(SIZE_DICT).astype(str).astype(int)\n",
    "\n",
    "    # dropping unknown values in gender\n",
    "    df.drop(df[df['gender'] == 'Unknown'].index, inplace=True)\n",
    "\n",
    "    # Convert binary columns to binary (0/1) data type\n",
    "    df[BINARY_COLS] = df[BINARY_COLS].astype(bool).astype(int)\n",
    "\n",
    "    # # Replace 'Male' and 'Female' with 0 and 1, respectively\n",
    "\n",
    "    # # Replace 'Male' and 'Female' with 0 and 1, respectively, and convert to int\n",
    "    # df['gender'] = df['gender'].replace({\"Male\": 0, \"Female\": 1}).astype(int)\n",
    "\n",
    "    # Filter data for los 1+\n",
    "    df = df[df['los'] >= 1]\n",
    "\n",
    "    # target encoding on larger categorical features\n",
    "    te = ce.TargetEncoder(cols=TARGET_COLS)\n",
    "    df[TARGET_COLS] = te.fit_transform(df[TARGET_COLS], df[\"los\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_mode(df, reference_column, feature):\n",
    "    # Calculate the mode coat for each breed_primary\n",
    "    mode_by_breed = df.groupby(reference_column)[feature].apply(lambda x: x.mode().iloc[0] if not x.isnull().all() else None)\n",
    "\n",
    "    # Create a dictionary mapping each breed to its mode coat\n",
    "    mode_dict = dict(mode_by_breed)\n",
    "\n",
    "    # Fill the NaN values in 'coat' based on the breed using the mode_dict\n",
    "    df[feature] = df.apply(lambda row: mode_dict[row[reference_column]] if pd.isna(row[feature]) and row[reference_column] in mode_dict else row[feature], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with null coat and color primary\n",
    "def drop_null_rows(df, feature):\n",
    "    df.dropna(subset=[feature], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "def remove_outliers(df, columns, zscore_threshold=3):\n",
    "    for col in columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        z_scores = np.abs((df[col] - mean) / std)\n",
    "        df = df[z_scores <= zscore_threshold]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing data\n",
    "def split_data(X, y, test_size = .33, random_state=312):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Randomized Search\n",
    "def perform_randomized_search(model, param_distributions, X_train, y_train, scoring='r2', cv=5):\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_distributions, scoring=scoring, cv=cv, n_jobs=-1, random_state=0)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    # best_model = random_search.best_estimator_\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and eval models\n",
    "\n",
    "def train_eval_models(X_train, X_test, y_train, y_test, model):\n",
    "    # Create a pipeline to scale the features and initialize the model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation with additional metrics\n",
    "    scores_r2 = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
    "    scores_mae = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "    scores_mse = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "    mean_score_r2 = scores_r2.mean()\n",
    "    mean_score_mae = -scores_mae.mean()\n",
    "    mean_score_mse = -scores_mse.mean()\n",
    "\n",
    "    # Model name and store results with each model\n",
    "    name = model.__class__.__name__\n",
    "    print('{} done. Mean R-squared (CV): {:.2f}, Mean MAE (CV): {:.2f}, Mean MSE (CV): {:.2f}'.format(\n",
    "        name, mean_score_r2, mean_score_mae, mean_score_mse))\n",
    "\n",
    "    # Train the best model on the entire training set and evaluate on the test set\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print('R-squared (test set): {:.2f}'.format(r2_score(y_test, y_test_pred)))\n",
    "    print('Mean squared error (test set): {:.2f}'.format(mean_squared_error(y_test, y_test_pred)))\n",
    "    print('Mean absolute error (test set): {:.2f}'.format(mean_absolute_error(y_test, y_test_pred)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the database\n",
    "df_raw = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_preprocessed = preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "outlier_columns = ['organization_name', 'los', 'breed_primary']\n",
    "df_no_outliers = remove_outliers(df_preprocessed, outlier_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_nan_mode(df_no_outliers, 'breed_primary', 'coat')\n",
    "df = fill_nan_mode(df_no_outliers, 'breed_primary', 'color_primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_null_rows(df, 'coat')\n",
    "df = drop_null_rows(df, 'color_primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = df.drop('los', axis = 1)\n",
    "y = df['los']\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_params = {\n",
    "    RandomForestRegressor: {'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'random_state': [0]},\n",
    "    GradientBoostingRegressor: {'n_estimators': [10, 50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0], 'random_state': [0]},\n",
    "    XGBRegressor: {'n_estimators': [10, 50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0], 'random_state': [0]},\n",
    "    LGBMRegressor: {'n_estimators': [10, 50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# saves the best estimator found by randomized search for each model with key being the model name\n",
    "best_models = {}\n",
    "for model, params in models_params.items():\n",
    "    randomized_search = perform_randomized_search(model(), params, X_train, y_train)\n",
    "    best_model = randomized_search.best_estimator_\n",
    "    best_models[model.__name__] = best_model\n",
    "    train_eval_models(X_train, X_test, y_train, y_test, best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stacking: defining the estimators it pulls models from the best_models dictionary\n",
    "estimators = [\n",
    "    ('ridge', Ridge()),\n",
    "    ('rf', best_models['RandomForestRegressor']),\n",
    "    ('gb', best_models['GradientBoostingRegressor']),\n",
    "    ('xgb', best_models['XGBRegressor']),\n",
    "    ('lgbm', best_models['LGBMRegressor'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge is used to help with overfitting\n",
    "final_model = StackingRegressor(estimators=estimators, final_estimator=Ridge(), cv=5)\n",
    "_ = train_eval_models(X_train, X_test, y_train, y_test, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training and validation on entire dataset\n",
    "final_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_petfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
