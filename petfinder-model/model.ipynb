{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "#from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge, ElasticNet\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEROKU_URL = os.getenv('HEROKU_POSTGRESQL_AMBER_URL')\n",
    "\n",
    "uri = HEROKU_URL \n",
    "if uri.startswith(\"postgres://\"):\n",
    "    uri = uri.replace(\"postgres://\", \"postgresql://\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "def read_data():\n",
    "    df_raw = df_raw = pd.read_sql('petfinder_with_dates', uri)  \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "def feature_drop(df):\n",
    "    df = df.drop(columns=[\"id\", \"name\", \"organization_id\", \"published_at\", \"status_changed_at\", \"attribute_declawed\", \"city\", \"state\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling missing values\n",
    "\n",
    "def handling_missing_values(df):\n",
    "    # Fill NaN values in 'age' column with 'Unknown'\n",
    "    df['age'].fillna('Unknown', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # transform \"age\" column mapping age and size\n",
    "    age_dict={\n",
    "    'Baby':'0',\n",
    "    'Young':'1',\n",
    "    'Adult':'2',\n",
    "    'Senior':'3'\n",
    "    }\n",
    "    df['age'] = df['age'].map(age_dict).astype(str).astype(int)\n",
    "\n",
    "    # transform \"size\" column\n",
    "    size_dict={\n",
    "    'Small':'0',\n",
    "    'Medium':'1',\n",
    "    'Large': '2',\n",
    "    'Extra Large': '3'\n",
    "    }\n",
    "    df['size'] = df['size'].map(size_dict).astype(str).astype(int)\n",
    "\n",
    "    # Convert binary columns to binary (0/1) data type\n",
    "    binary_cols = [\"breed_mixed\", \"breed_unknown\", \"good_with_children\", \"good_with_dogs\", \"good_with_cats\", \"attribute_spayed_neutered\",\n",
    "                   \"attribute_house_trained\", \"attribute_shots_current\", \"attribute_special_needs\"]\n",
    "    df[binary_cols] = df[binary_cols].astype(bool).astype(int)\n",
    "\n",
    "    # Replace 'Male' and 'Female' with 0 and 1, respectively\n",
    "    df['gender'] = df['gender'].replace({\"Male\": 0, \"Female\": 1})\n",
    "\n",
    "    # Compute the mode of the 'gender' column, ignoring 'Unknown'\n",
    "    mode = df.loc[df['gender'] != 'Unknown', 'gender'].mode()[0]\n",
    "\n",
    "    # Replace 'Unknown' values with the mode\n",
    "    df['gender'] = df['gender'].replace({'Unknown': mode})\n",
    "\n",
    "    # target encoding on larger categorical features\n",
    "    target_cols = [\"coat\", \"organization_name\", \"breed_primary\", \"breed_secondary\", \"color_primary\", \"color_secondary\", \"color_tertiary\"]\n",
    "    te = ce.TargetEncoder(cols=target_cols)\n",
    "    df[target_cols] = te.fit_transform(df[target_cols], df[\"los\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "def remove_outliers(df, columns, zscore_threshold=3):\n",
    "    for col in columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        z_scores = np.abs((df[col] - mean) / std)\n",
    "        df = df[z_scores <= zscore_threshold]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_correlated(df, target, threshold):\n",
    "    # Exclude the target variable from correlation analysis\n",
    "    features = df.drop(target, axis=1)\n",
    "    \n",
    "    corr_matrix = features.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    reduced_corr_matrix = corr_matrix.mask(mask)\n",
    "    features_to_drop = [c for c in reduced_corr_matrix.columns if any(reduced_corr_matrix[c] > threshold)]\n",
    "    return features_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing data\n",
    "def split_data(X, y, test_size = .33, random_state=312):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature selection using RandomForestRegressor\n",
    "# def feature_selection(X_train, y_train, threshold=0.01):\n",
    "#     rf = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "#     rf.fit(X_train, y_train)\n",
    "\n",
    "#     # Select features with importance greater than 0.01\n",
    "#     selector = SelectFromModel(rf, threshold=threshold, prefit=True)\n",
    "#     X_train_important = selector.transform(X_train)\n",
    "    \n",
    "#     return X_train_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Randomized Search\n",
    "def perform_randomized_search(model, param_distributions, X_train, y_train, scoring='r2', cv=5):\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_distributions, n_iter=50, scoring=scoring, cv=cv, n_jobs=-1, random_state=0)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "def perform_hyperparameter_tuning(pipeline, param_grid, X_train, y_train, cv=3):\n",
    "    randomized_search = RandomizedSearchCV(pipeline, param_grid, n_iter=10, cv=cv, n_jobs=-1, random_state=1)\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "    best_params = randomized_search.best_params_\n",
    "    best_score = randomized_search.best_score_\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and eval models\n",
    "\n",
    "def train_eval_models(X_train, X_test, y_train, y_test, models):\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        # Create a pipeline to scale the features and initialize the model\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', clone(model))\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation with additional metrics\n",
    "        scores_r2 = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
    "        scores_mae = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "        scores_mse = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "        mean_score_r2 = scores_r2.mean()\n",
    "        mean_score_mae = -scores_mae.mean()\n",
    "        mean_score_mse = -scores_mse.mean()\n",
    "\n",
    "        # Model name and store results with each model\n",
    "        name = model.__class__.__name__\n",
    "        results[name] = (mean_score_r2, mean_score_mae, mean_score_mse)\n",
    "        print('{} done. Mean R-squared (CV): {:.2f}, Mean MAE (CV): {:.2f}, Mean MSE (CV): {:.2f}'.format(\n",
    "            name, mean_score_r2, mean_score_mae, mean_score_mse))\n",
    "        \n",
    "    # Train the best model on the entire training set and evaluate on the test set\n",
    "\n",
    "    best_model = max(results, key=results.get)\n",
    "    best_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', clone(models[models.index(best_model)]))\n",
    "    ])\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "    print('\\nBest model: {}'.format(best_model))\n",
    "    print('R-squared (test set): {:.2f}'.format(r2_score(y_test, y_test_pred)))\n",
    "    print('Mean squared error (test set): {:.2f}'.format(mean_squared_error(y_test, y_test_pred)))\n",
    "    print('Mean absolute error (test set): {:.2f}'.format(mean_absolute_error(y_test, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_eval_models_important_features(X_train_important, X_test_important, y_train, y_test, models):\n",
    "#     # Retrain models with important features\n",
    "#     results_important = {}\n",
    "\n",
    "#     for model in models:\n",
    "#         # Initialize the models\n",
    "#         regr = model\n",
    "#         regr.fit(X_train_important, y_train)\n",
    "        \n",
    "#         # Predictions from models\n",
    "#         y_test_pred = regr.predict(X_test_important)\n",
    "        \n",
    "#         # Model name and stored results with each model\n",
    "#         name = model.__class__.__name__\n",
    "        \n",
    "#         results_important[name] = r2_score(y_test, y_test_pred)\n",
    "#         print('{} done. R-squared (important features): {:.2f}'.format(name, results_important[name]))\n",
    "\n",
    "#     # Find the best model\n",
    "#     best_model = max(results_important, key=results_important.get)\n",
    "#     best_r2 = results_important[best_model]\n",
    "\n",
    "#     # Evaluate the best model\n",
    "#     best_regr = models[[m.__class__.__name__ for m in models].index(best_model)]\n",
    "#     y_test_pred = best_regr.predict(X_test_important)\n",
    "\n",
    "#     mse = mean_squared_error(y_test, y_test_pred)\n",
    "#     mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "#     print('\\nBest model: {}'.format(best_model))\n",
    "#     print('R-squared (test set, important features): {:.2f}'.format(best_r2))\n",
    "#     print('Mean squared error (test set, important features): {:.2f}'.format(mse))\n",
    "#     print('Mean absolute error (test set, important features): {:.2f}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the database\n",
    "df_raw = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature drop\n",
    "df_raw = feature_drop(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df_raw = handling_missing_values(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_preprocessed = preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "outlier_columns = ['organization_name', 'los', 'breed_primary', 'breed_secondary']\n",
    "df_no_outliers = remove_outliers(df_preprocessed, outlier_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify correlated features\n",
    "correlation_threshold = 0.2\n",
    "to_drop = identify_correlated(df_no_outliers, target=\"los\", threshold=correlation_threshold)\n",
    "df = df_no_outliers.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = df.drop('los', axis = 1)\n",
    "y = df['los']\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature selection\n",
    "# feature_selection_threshold = 0.01\n",
    "# X_train_important = feature_selection(X_train,y_train, feature_selection)\n",
    "# X_test_important = feature_selection(X_test, y_test, feature_selection_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = [SGDRegressor(random_state=0), \n",
    "          GradientBoostingRegressor(random_state=0), \n",
    "          LinearRegression(),\n",
    "          Lasso(random_state=0),\n",
    "          Ridge(random_state=0),\n",
    "          ElasticNet(random_state=0),\n",
    "          DecisionTreeRegressor(random_state=0),\n",
    "          RandomForestRegressor(n_estimators=100, random_state=0),\n",
    "          XGBRegressor(),\n",
    "          LGBMRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform randomizedsearchCV for XGBRegressor\n",
    "xgb_param_distributions = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# best_xgb = perform_randomized_search(XGBRegressor(random_state=0), xgb_param_distributions,\n",
    "#                                      X_train_important, y_train)\n",
    "\n",
    "best_xgb = perform_randomized_search(XGBRegressor(random_state=0), xgb_param_distributions,\n",
    "                                     X_train, y_train)\n",
    "\n",
    "\n",
    "# Include best_xgb in the models list\n",
    "models.append(best_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for RandomForestRegressor\n",
    "params_rf = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# best_rf_params, _ = perform_hyperparameter_tuning(RandomForestRegressor(random_state=0), params_rf,\n",
    "#                                                   X_train_important, y_train)\n",
    "best_rf_params, _ = perform_hyperparameter_tuning(RandomForestRegressor(random_state=0), params_rf,\n",
    "                                                    X_train, y_train)\n",
    "\n",
    "\n",
    "# Initialize the best_rf model with the RandomForestRegressor model using the best parameters\n",
    "best_rf = RandomForestRegressor(**best_rf_params, random_state=0)\n",
    "\n",
    "# Include best_rf in the models list\n",
    "models.append(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for GradientBoostingRegressor\n",
    "\n",
    "gbr_param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__min_samples_split': [2, 3, 4],\n",
    "    'model__min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "gbr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# best_gbr_params, best_gbr_score = perform_hyperparameter_tuning(gbr_pipeline, gbr_param_grid, \n",
    "#                                                                 X_train_important, y_train)\n",
    "best_gbr_params, best_gbr_score = perform_hyperparameter_tuning(gbr_pipeline, gbr_param_grid, \n",
    "                                                                X_train, y_train)\n",
    "# Update the initialization of gbr_pipeline with the best parameters\n",
    "gbr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', GradientBoostingRegressor(**best_gbr_params))\n",
    "])\n",
    "\n",
    "# Include gbr_pipeline in the models list\n",
    "models.append(gbr_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for LGBMRegressor\n",
    "lgbm_param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__num_leaves': [31, 45, 60],\n",
    "    'model__min_child_samples': [20, 30, 40]\n",
    "}\n",
    "\n",
    "lgbm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "\n",
    "# best_lgbm_params, best_lgbm_score = perform_hyperparameter_tuning(lgbm_pipeline, lgbm_param_grid, \n",
    "#                                                                   X_train_important, y_train)\n",
    "best_lgbm_params, best_lgbm_score = perform_hyperparameter_tuning(lgbm_pipeline, lgbm_param_grid, \n",
    "                                                                  X_train, y_train)\n",
    "\n",
    "print('LGBMRegressor best parameters: ', best_lgbm_params)\n",
    "print('LGBMRegressor best score: {:.2f}'.format(best_lgbm_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stacking\n",
    "estimators = [\n",
    "    ('ridge', Ridge()),\n",
    "    ('lasso', Lasso()),\n",
    "    ('elastic_net', ElasticNet()),\n",
    "    ('gbm', GradientBoostingRegressor()),\n",
    "    ('xgb', XGBRegressor()),\n",
    "    ('lgbm', LGBMRegressor())\n",
    "]\n",
    "stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
    "models.append(stacking_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and evaluate models\n",
    "# train_eval_models(X_train_important, X_test_important, y_train, y_test, models)\n",
    "train_eval_models(X_train, X_test, y_train, y_test, models)\n",
    "\n",
    "# # Retrain models with important features and evaluate again\n",
    "# train_eval_models_important_features(X_train_important, X_test_important, y_train, y_test, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4b8b30545f5d65175c89d8b33afca65e1f9aa03e972311ddac705e77798068f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
