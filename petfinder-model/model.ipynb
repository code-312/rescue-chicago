{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge, ElasticNet\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEROKU_URL = os.getenv('HEROKU_POSTGRESQL_AMBER_URL')\n",
    "\n",
    "uri = HEROKU_URL \n",
    "if uri.startswith(\"postgres://\"):\n",
    "    uri = uri.replace(\"postgres://\", \"postgresql://\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "def read_data():\n",
    "    df_raw = df_raw = pd.read_sql('petfinder_with_dates', uri)  \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "def feature_drop(df):\n",
    "    df = df.drop(columns=[\"id\", \"name\", \"organization_id\", \"published_at\", \"status_changed_at\", \"attribute_declawed\", \"city\", \"state\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling missing values\n",
    "\n",
    "def handling_missing_values(df):\n",
    "    # Fill NaN values in 'age' column with 'Unknown'\n",
    "    df['age'].fillna('Unknown', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # transform \"age\" column mapping age and size\n",
    "    age_dict={\n",
    "    'Baby':'0',\n",
    "    'Young':'1',\n",
    "    'Adult':'2',\n",
    "    'Senior':'3'\n",
    "    }\n",
    "    df['age'] = df['age'].map(age_dict).astype(str).astype(int)\n",
    "\n",
    "    # transform \"size\" column\n",
    "    size_dict={\n",
    "    'Small':'0',\n",
    "    'Medium':'1',\n",
    "    'Large': '2',\n",
    "    'Extra Large': '3'\n",
    "    }\n",
    "    df['size'] = df['size'].map(size_dict).astype(str).astype(int)\n",
    "\n",
    "    # Convert binary columns to binary (0/1) data type\n",
    "    binary_cols = [\"breed_mixed\", \"breed_unknown\", \"good_with_children\", \"good_with_dogs\", \"good_with_cats\", \"attribute_spayed_neutered\",\n",
    "                   \"attribute_house_trained\", \"attribute_shots_current\", \"attribute_special_needs\"]\n",
    "    df[binary_cols] = df[binary_cols].astype(bool).astype(int)\n",
    "\n",
    "    # Replace 'Male' and 'Female' with 0 and 1, respectively\n",
    "    df['gender'] = df['gender'].replace({\"Male\": 0, \"Female\": 1})\n",
    "\n",
    "    # Compute the mode of the 'gender' column, ignoring 'Unknown'\n",
    "    mode = df.loc[df['gender'] != 'Unknown', 'gender'].mode()[0]\n",
    "\n",
    "    # Replace 'Unknown' values with the mode\n",
    "    df['gender'] = df['gender'].replace({'Unknown': mode})\n",
    "\n",
    "    # target encoding on larger categorical features\n",
    "    target_cols = [\"coat\", \"organization_name\", \"breed_primary\", \"breed_secondary\", \"color_primary\", \"color_secondary\", \"color_tertiary\"]\n",
    "    te = ce.TargetEncoder(cols=target_cols)\n",
    "    df[target_cols] = te.fit_transform(df[target_cols], df[\"los\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "def remove_outliers(df, columns, zscore_threshold=3):\n",
    "    for col in columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        z_scores = np.abs((df[col] - mean) / std)\n",
    "        df = df[z_scores <= zscore_threshold]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_correlated(df, target, threshold):\n",
    "    # Exclude the target variable from correlation analysis\n",
    "    features = df.drop(target, axis=1)\n",
    "    \n",
    "    corr_matrix = features.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    reduced_corr_matrix = corr_matrix.mask(mask)\n",
    "    features_to_drop = [c for c in reduced_corr_matrix.columns if any(reduced_corr_matrix[c] > threshold)]\n",
    "    return features_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing data\n",
    "def split_data(X, y, test_size = .33, random_state=312):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using RandomForestRegressor\n",
    "def feature_selection(X_train, y_train, threshold=0.01):\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Select features with importance greater than 0.01\n",
    "    selector = SelectFromModel(rf, threshold=threshold, prefit=True)\n",
    "    X_train_important = selector.transform(X_train)\n",
    "    \n",
    "    return X_train_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Randomized Search\n",
    "def perform_randomized_search(model, param_distributions, X_train, y_train, scoring='r2', cv=5):\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_distributions, n_iter=50, scoring=scoring, cv=cv, n_jobs=-1, random_state=0)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "def perform_hyperparameter_tuning(pipeline, param_grid, X_train, y_train, cv=3):\n",
    "    randomized_search = RandomizedSearchCV(pipeline, param_grid, n_iter=10, cv=cv, n_jobs=-1, random_state=1)\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "    best_params = randomized_search.best_params_\n",
    "    best_score = randomized_search.best_score_\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and eval models\n",
    "\n",
    "def train_eval_models(X_train, X_test, y_train, y_test, models):\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        \n",
    "        # Create a pipeline to scale the features and initialize the model\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation with additional metrics\n",
    "        scores_r2 = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
    "        scores_mae = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "        scores_mse = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "        mean_score_r2 = scores_r2.mean()\n",
    "        mean_score_mae = -scores_mae.mean()\n",
    "        mean_score_mse = -scores_mse.mean()\n",
    "\n",
    "        # Model name and store results with each model\n",
    "        name = model.__class__.__name__\n",
    "        results[name] = (mean_score_r2, mean_score_mae, mean_score_mse)\n",
    "        print('{} done. Mean R-squared (CV): {:.2f}, Mean MAE (CV): {:.2f}, Mean MSE (CV): {:.2f}'.format(\n",
    "            name, mean_score_r2, mean_score_mae, mean_score_mse))\n",
    "        \n",
    "    # Train the best model on the entire training set and evaluate on the test set\n",
    "\n",
    "    best_model = max(results, key=results.get)\n",
    "    best_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', clone(eval(best_model)))\n",
    "    ])\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "    print('\\nBest model: {}'.format(best_model))\n",
    "    print('R-squared (test set): {:.2f}'.format(r2_score(y_test, y_test_pred)))\n",
    "    print('Mean squared error (test set): {:.2f}'.format(mean_squared_error(y_test, y_test_pred)))\n",
    "    print('Mean absolute error (test set): {:.2f}'.format(mean_absolute_error(y_test, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_models_important_features(X_train_important, X_test_important, y_train, y_test, models):\n",
    "    # Retrain models with important features\n",
    "    results_important = {}\n",
    "\n",
    "    for model in models:\n",
    "        # Initialize the models\n",
    "        regr = model\n",
    "        regr.fit(X_train_important, y_train)\n",
    "        \n",
    "        # Predictions from models\n",
    "        y_test_pred = regr.predict(X_test_important)\n",
    "        \n",
    "        # Model name and stored results with each model\n",
    "        name = model.__class__.__name__\n",
    "        \n",
    "        results_important[name] = r2_score(y_test, y_test_pred)\n",
    "        print('{} done. R-squared (important features): {:.2f}'.format(name, results_important[name]))\n",
    "\n",
    "    # Find the best model\n",
    "    best_model = max(results_important, key=results_important.get)\n",
    "    best_r2 = results_important[best_model]\n",
    "\n",
    "    # Evaluate the best model\n",
    "    best_regr = models[[m.__class__.__name__ for m in models].index(best_model)]\n",
    "    y_test_pred = best_regr.predict(X_test_important)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_test_pred)\n",
    "    mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    print('\\nBest model: {}'.format(best_model))\n",
    "    print('R-squared (test set, important features): {:.2f}'.format(best_r2))\n",
    "    print('Mean squared error (test set, important features): {:.2f}'.format(mse))\n",
    "    print('Mean absolute error (test set, important features): {:.2f}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the database\n",
    "df_raw = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature drop\n",
    "df_raw = feature_drop(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df_raw = handling_missing_values(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_preprocessed = preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "outlier_columns = ['organization_name', 'los', 'breed_primary', 'breed_secondary']\n",
    "df_no_outliers = remove_outliers(df_preprocessed, outlier_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify correlated features\n",
    "correlation_threshold = 0.2\n",
    "to_drop = identify_correlated(df_no_outliers, target=\"los\", threshold=correlation_threshold)\n",
    "df = df_no_outliers.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                        int64\n",
       "gender                       int64\n",
       "organization_name          float64\n",
       "los                          int64\n",
       "breed_primary              float64\n",
       "breed_mixed                  int64\n",
       "breed_unknown                int64\n",
       "color_secondary            float64\n",
       "color_tertiary             float64\n",
       "good_with_cats               int64\n",
       "attribute_house_trained      int64\n",
       "attribute_special_needs      int64\n",
       "attribute_shots_current      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = df.drop('los', axis = 1)\n",
    "y = df['los']\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Feature selection\u001b[39;00m\n\u001b[1;32m      2\u001b[0m feature_selection_threshold \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m----> 3\u001b[0m X_train_important \u001b[39m=\u001b[39m feature_selection(X_train,y_train, feature_selection)\n\u001b[1;32m      4\u001b[0m X_test_important \u001b[39m=\u001b[39m feature_selection(X_test, y_test, feature_selection_threshold)\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mfeature_selection\u001b[0;34m(X_train, y_train, threshold)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Select features with importance greater than 0.01\u001b[39;00m\n\u001b[1;32m      7\u001b[0m selector \u001b[39m=\u001b[39m SelectFromModel(rf, threshold\u001b[39m=\u001b[39mthreshold, prefit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m X_train_important \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mtransform(X_train)\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m X_train_important\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv_petfinder/lib/python3.9/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv_petfinder/lib/python3.9/site-packages/sklearn/feature_selection/_base.py:90\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# public Mixin.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m     84\u001b[0m     X,\n\u001b[1;32m     85\u001b[0m     dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     89\u001b[0m )\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv_petfinder/lib/python3.9/site-packages/sklearn/feature_selection/_base.py:94\u001b[0m, in \u001b[0;36mSelectorMixin._transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m     93\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Reduce X to the selected features.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_support()\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m     96\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     97\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNo features were selected: either the data is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m too noisy or the selection test too strict.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m    100\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv_petfinder/lib/python3.9/site-packages/sklearn/feature_selection/_base.py:53\u001b[0m, in \u001b[0;36mSelectorMixin.get_support\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_support\u001b[39m(\u001b[39mself\u001b[39m, indices\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    Get a mask, or integer index, of the features selected.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m        values are indices into the input feature vector.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_support_mask()\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m mask \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indices \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mwhere(mask)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv_petfinder/lib/python3.9/site-packages/sklearn/feature_selection/_from_model.py:295\u001b[0m, in \u001b[0;36mSelectFromModel._get_support_mask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`max_features` must be an integer. Got `max_features=\u001b[39m\u001b[39m{\u001b[39;00mmax_features\u001b[39m}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minstead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    289\u001b[0m scores \u001b[39m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    290\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    291\u001b[0m     getter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimportance_getter,\n\u001b[1;32m    292\u001b[0m     transform_func\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    293\u001b[0m     norm_order\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_order,\n\u001b[1;32m    294\u001b[0m )\n\u001b[0;32m--> 295\u001b[0m threshold \u001b[39m=\u001b[39m _calculate_threshold(estimator, scores, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthreshold)\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(scores, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv_petfinder/lib/python3.9/site-packages/sklearn/feature_selection/_from_model.py:65\u001b[0m, in \u001b[0;36m_calculate_threshold\u001b[0;34m(estimator, importances, threshold)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected threshold=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or threshold=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m'\u001b[39m\u001b[39m got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m threshold\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     64\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     threshold \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39;49m(threshold)\n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m threshold\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'function'"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "feature_selection_threshold = 0.01\n",
    "X_train_important = feature_selection(X_train,y_train, feature_selection)\n",
    "X_test_important = feature_selection(X_test, y_test, feature_selection_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = [SGDRegressor(random_state=0), \n",
    "          GradientBoostingRegressor(random_state=0), \n",
    "          LinearRegression(),\n",
    "          Lasso(random_state=0),\n",
    "          Ridge(random_state=0),\n",
    "          ElasticNet(random_state=0),\n",
    "          DecisionTreeRegressor(random_state=0),\n",
    "          RandomForestRegressor(n_estimators=100, random_state=0),\n",
    "          XGBRegressor(),\n",
    "          LGBMRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform randomizedsearchCV for XGBRegressor\n",
    "xgb_param_distributions = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "best_xgb = perform_randomized_search(XGBRegressor(random_state=0), xgb_param_distributions,\n",
    "                                     X_train_important, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for RandomForestRegressor\n",
    "params_rf = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "best_rf_params, _ = perform_hyperparameter_tuning(RandomForestRegressor(random_state=0), params_rf,\n",
    "                                                  X_train_important, y_train)\n",
    "\n",
    "best_rf = RandomForestRegressor(**best_rf_params, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for GradientBoostingRegressor\n",
    "\n",
    "gbr_param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__min_samples_split': [2, 3, 4],\n",
    "    'model__min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "gbr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "best_gbr_params, best_gbr_score = perform_hyperparameter_tuning(gbr_pipeline, gbr_param_grid, \n",
    "                                                                X_train_important, y_train)\n",
    "print('GradientBoostingRegressor best parameters: ', best_gbr_params)\n",
    "print('GradientBoostingRegressor best score: {:.2f}'.format(best_gbr_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for LGBMRegressor\n",
    "lgbm_param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__num_leaves': [31, 45, 60],\n",
    "    'model__min_child_samples': [20, 30, 40]\n",
    "}\n",
    "\n",
    "lgbm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "\n",
    "best_lgbm_params, best_lgbm_score = perform_hyperparameter_tuning(lgbm_pipeline, lgbm_param_grid, \n",
    "                                                                  X_train_important, y_train)\n",
    "\n",
    "print('LGBMRegressor best parameters: ', best_lgbm_params)\n",
    "print('LGBMRegressor best score: {:.2f}'.format(best_lgbm_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stacking\n",
    "estimators = [\n",
    "    ('ridge', Ridge()),\n",
    "    ('lasso', Lasso()),\n",
    "    ('elastic_net', ElasticNet()),\n",
    "    ('gbm', GradientBoostingRegressor()),\n",
    "    ('xgb', XGBRegressor()),\n",
    "    ('lgbm', LGBMRegressor())\n",
    "]\n",
    "stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
    "models.append(stacking_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "train_eval_models(X_train_important, X_test_important, y_train, y_test, models)\n",
    "\n",
    "# Retrain models with important features and evaluate again\n",
    "train_eval_models_important_features(X_train_important, X_test_important, y_train, y_test, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4b8b30545f5d65175c89d8b33afca65e1f9aa03e972311ddac705e77798068f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
